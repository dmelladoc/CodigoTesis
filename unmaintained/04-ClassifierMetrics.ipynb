{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of classifier\n",
    "\n",
    "We can load a model and check its metrics.\n",
    "In this one, we will load our trained model and obtain metrics about the classification task.\n",
    "\n",
    "(we will also compare the original model and check its metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from FindClf import Dataset, Models\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchmetrics.classification import (\n",
    "    MultilabelAccuracy,\n",
    "    MultilabelPrecision,\n",
    "    MultilabelStatScores,\n",
    "    MultilabelRecall,\n",
    "    MultilabelF1Score,\n",
    "    MultilabelROC,\n",
    "    MultilabelAUROC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "imagepath = \"\"  # Image directory with vindr Dataset images processed with our method\n",
    "csvpath = (\n",
    "    \"finding_annotations_V2.csv\"  # Grouped annotations for asymmetries and retractions\n",
    ")\n",
    "label_names = [\n",
    "    \"No Finding\",\n",
    "    \"Mass\",\n",
    "    \"Suspicious Calcification\",\n",
    "    \"Asymmetries\",\n",
    "    \"Architectural Distortion\",\n",
    "    \"Suspicious Lymph Node\",\n",
    "    \"Skin Thickening\",\n",
    "    \"Retractions\",\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "scales = (0.05, 5.0)\n",
    "ratios = (0.33, 1.66)\n",
    "window_size = (256, 256)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "njobs = 16\n",
    "seed = 348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will set a seed for reproducibility when evaluating both models\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "## Semilla (para reproducibilidad)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transforms\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            window_size,\n",
    "            interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "            antialias=True,\n",
    "        ),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv\n",
    "df = pd.read_csv(csvpath)\n",
    "df_test = df.groupby(\"split\").get_group(\"test\")\n",
    "test_dataset = Dataset.VindrDataset(df_test, imagepath, test_transforms, stage=\"test\")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=njobs,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the model\n",
    "model_path = \"\"  # path to the model checkpoint\n",
    "state_dict = torch.load(model_path, weights_only=True, map_location=device)\n",
    "\n",
    "model = Models.create_efficientNetV2(len(label_names))\n",
    "model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all metrics\n",
    "nlabels = len(label_names)\n",
    "gral_metrics = {\n",
    "    \"Accuracy\": MultilabelAccuracy(\n",
    "        num_labels=nlabels, average=\"weighted\", ignore_index=0\n",
    "    ),\n",
    "    \"Precision\": MultilabelPrecision(\n",
    "        num_labels=nlabels, average=\"weighted\", ignore_index=0\n",
    "    ),\n",
    "    \"Recall\": MultilabelRecall(num_labels=nlabels, average=\"weighted\", ignore_index=0),\n",
    "    \"F1\": MultilabelF1Score(num_labels=nlabels, average=\"weighted\", ignore_index=0),\n",
    "    \"AUROC\": MultilabelAUROC(num_labels=nlabels, average=\"weighted\"),\n",
    "}\n",
    "class_metrics = {\n",
    "    \"Accuracy\": MultilabelAccuracy(num_labels=nlabels, average=None),\n",
    "    \"Precision\": MultilabelPrecision(num_labels=nlabels, average=None),\n",
    "    \"Recall\": MultilabelRecall(num_labels=nlabels, average=None),\n",
    "    \"F1\": MultilabelF1Score(num_labels=nlabels, average=None),\n",
    "    \"AUROC\": MultilabelAUROC(num_labels=nlabels, average=None),\n",
    "    \"StatScores\": MultilabelStatScores(num_labels=nlabels, average=None),\n",
    "}\n",
    "\n",
    "[metric.to(device) for metric in gral_metrics.values()]\n",
    "[metric.to(device) for metric in class_metrics.values()]\n",
    "\n",
    "print(\"Metrics loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yreal, predictions = [], []\n",
    "\n",
    "with tqdm(total=len(test_loader), desc=\"Evaluating\") as pbar:\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred = torch.sigmoid(outputs[\"Classifier\"])\n",
    "\n",
    "            for metric in gral_metrics.values():\n",
    "                metric.update(outputs[\"Classifier\"], labels.int())\n",
    "\n",
    "            for metric in class_metrics.values():\n",
    "                metric.update(pred, labels.int())\n",
    "\n",
    "            yreal.append(labels.detach().cpu().numpy())\n",
    "            predictions.append(pred.detach().cpu().numpy())\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general Metrics\n",
    "for name, metric in gral_metrics.items():\n",
    "    print(f\"{name}: {metric.compute():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = class_metrics[\"StatScores\"].compute()\n",
    "scores[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyreal = np.concatenate(yreal, axis=0)\n",
    "yypred = np.concatenate(predictions, axis=0)\n",
    "print(yyreal.shape, yypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = metrics.multilabel_confusion_matrix(yyreal, yypred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.diagonal(axis1=1, axis2=2) / cms.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Metrics\")\n",
    "print(\n",
    "    f\"{'Labels':>30}: {'Accuracy':^10} {'Precision':^10} {'Recall':^10} {'F1':^10} {'AUROC':^10} {'Support':^10}\"\n",
    ")\n",
    "for i, label in enumerate(label_names):\n",
    "    print(f\"{label:>30}:\", end=\" \")\n",
    "    for metric in [\n",
    "        \"Accuracy\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1\",\n",
    "        \"AUROC\",\n",
    "    ]:\n",
    "        print(f\"{class_metrics[metric].compute()[i]:^10.4f}\", end=\" \")\n",
    "    print(f\"{scores[i, -1].item():^10d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_label_names = [\n",
    "    \"Sin Hallazgo\",\n",
    "    \"Masa\",\n",
    "    \"Calcificación Sospechosa\",\n",
    "    \"Asimetrías\",\n",
    "    \"Distorsión Arquitectura\",\n",
    "    \"Linfonodo Sospechoso\",\n",
    "    \"Engrosamiento de Piel\",\n",
    "    \"Retracciones\",\n",
    "]\n",
    "\n",
    "statsdf = pd.DataFrame(\n",
    "    index=pd.Index(es_label_names, name=\"Hallazgo\"),\n",
    "    columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\", \"Soporte\"],\n",
    ")\n",
    "\n",
    "for i, label in enumerate(es_label_names):\n",
    "    for metric in [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"]:\n",
    "        statsdf.loc[label, metric] = class_metrics[metric].compute()[i].item()\n",
    "    statsdf.loc[label, \"Soporte\"] = scores[i, -1].item()\n",
    "\n",
    "statsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = (\n",
    "    statsdf.style.format(\"{:.3%}\", subset=[\"Accuracy\", \"Precision\", \"Recall\"])\n",
    "    .format(\"{:.4f}\", subset=[\"F1\", \"AUROC\"])\n",
    "    .format(\"{:d}\", subset=[\"Soporte\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statsdf.to_markdown(tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamai2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
