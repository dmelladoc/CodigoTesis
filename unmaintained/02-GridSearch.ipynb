{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "\n",
    "In this notebook we do a grid search on the dataset for finetune some parameters \n",
    "\n",
    "More specifically, we will find learning rate, and the parameters for the focal loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "from FindClf import Dataset, Models\n",
    "\n",
    "from ray import tune\n",
    "from ray import train as Train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "imagepath = \"\"  # Path to images directory\n",
    "csvpath = (\n",
    "    \"finding_annotations_V2.csv\"  # Grouped annotations for asymmetries and retractions\n",
    ")\n",
    "label_names = [\n",
    "    \"No Finding\",\n",
    "    \"Mass\",\n",
    "    \"Suspicious Calcification\",\n",
    "    \"Asymmetries\",\n",
    "    \"Architectural Distortion\",\n",
    "    \"Suspicious Lymph Node\",\n",
    "    \"Skin Thickening\",\n",
    "    \"Retractions\",\n",
    "]\n",
    "\n",
    "batch_size = 48\n",
    "epochs = 10\n",
    "\n",
    "scales = (0.05, 5.0)\n",
    "ratios = (0.33, 1.66)\n",
    "window_size = (256, 256)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "njobs = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            window_size,\n",
    "            interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "            antialias=True,\n",
    "        ),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the dataset\n",
    "df = pd.read_csv(csvpath)\n",
    "traindf = df.groupby(\"split\").get_group(\"training\")\n",
    "\n",
    "# split training into training and validation with 80% and 20% respectively\n",
    "# This subset will have some images from the same patient, but for hyperparameter tuning we will ignore this\n",
    "df_train = traindf.sample(frac=0.8, random_state=42)\n",
    "df_val = traindf.drop(df_train.index)\n",
    "\n",
    "df_test = df.groupby(\"split\").get_group(\"test\")\n",
    "\n",
    "print(f\"Training: {len(df_train)}\")\n",
    "print(f\"Validation: {len(df_val)}\")\n",
    "print(f\"Test: {len(df_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset objects\n",
    "train_dataset = Dataset.VindrDataset(\n",
    "    df_train, imagepath, general_transforms, stage=\"train\"\n",
    ")\n",
    "val_dataset = Dataset.VindrDataset(df_val, imagepath, general_transforms, stage=\"val\")\n",
    "test_dataset = Dataset.VindrDataset(\n",
    "    df_test, imagepath, general_transforms, stage=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our training function for finding the hyperparams we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # create the model\n",
    "    model = Models.create_efficientNetV2(len(label_names))\n",
    "\n",
    "    # device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=config[\"lr\"], weight_decay=config[\"decay\"]\n",
    "    )\n",
    "\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            datapath = Path(ckpt_dir) / \"data.pkl\"\n",
    "            with open(datapath, \"rb\") as fp:\n",
    "                ckpt_state = pickle.load(fp)\n",
    "\n",
    "            start_epoch = ckpt_state[\"epoch\"]\n",
    "            model.load_state_dict(ckpt_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(ckpt_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=njobs,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=njobs,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = MultilabelAccuracy(\n",
    "        num_labels=len(label_names), average=\"macro\", ignore_index=0\n",
    "    )\n",
    "    accuracy.to(device)\n",
    "    f1score = MultilabelF1Score(\n",
    "        num_labels=len(label_names), average=\"macro\", ignore_index=0\n",
    "    )\n",
    "    f1score.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        accuracy.reset()\n",
    "        f1score.reset()\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = sigmoid_focal_loss(\n",
    "                outputs[\"Classifier\"],\n",
    "                labels,\n",
    "                alpha=config[\"alpha\"],\n",
    "                gamma=config[\"gamma\"],\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print(\n",
    "                    f\"[{epoch + 1:d}, {i + 1:5d}] loss: {running_loss / epoch_steps:.4f}\"\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                accuracy.update(outputs[\"Classifier\"], labels)\n",
    "                f1score.update(outputs[\"Classifier\"], labels)\n",
    "                # _, predicted = torch.max(outputs['Classifier'].data, 1)\n",
    "                # total += labels.size(0)\n",
    "                # correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = sigmoid_focal_loss(\n",
    "                    outputs[\"Classifier\"],\n",
    "                    labels,\n",
    "                    alpha=config[\"alpha\"],\n",
    "                    gamma=config[\"gamma\"],\n",
    "                    reduction=\"sum\",\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        val_acc = accuracy.compute().item()\n",
    "        val_f1 = f1score.compute().item()\n",
    "        print(\n",
    "            f\"Val Epoch {epoch + 1:d} loss: {val_loss / val_steps:.4f} accuracy: {val_acc:.4f} f1score: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        ckpt_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as ckpt_dir:\n",
    "            datapath = Path(ckpt_dir) / \"data.pkl\"\n",
    "            with open(datapath, \"wb\") as fp:\n",
    "                pickle.dump(ckpt_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(ckpt_dir)\n",
    "            Train.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": val_acc, \"f1score\": val_f1},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cuda\"):\n",
    "    testloader = DataLoader(\n",
    "        test_dataset, batch_size=16, shuffle=False, num_workers=njobs\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs[\"Classifier\"].data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config space\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"decay\": tune.loguniform(1e-6, 1e-3),\n",
    "    #'batch_size': tune.choice([8, 16, 32, 48]),\n",
    "    \"alpha\": tune.uniform(0, 1),\n",
    "    \"gamma\": tune.quniform(1.0, 5.0, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tune.run(\n",
    "    train_model,\n",
    "    resources_per_trial={\"cpu\": 24, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples=50,\n",
    "    scheduler=ASHAScheduler(\n",
    "        metric=\"f1score\", mode=\"max\", max_t=10, grace_period=2, reduction_factor=2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = result.get_best_trial(\"f1score\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best.config}\")\n",
    "print(f\"Best trial final validation loss: {best.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best.last_result['accuracy']}\")\n",
    "print(f\"Best trial final validation f1score: {best.last_result['f1score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = result.get_best_trial(\"f1score\", \"max\", \"last\")\n",
    "print(f\"Best trial config: {best.config}\")\n",
    "print(f\"Best trial final validation loss: {best.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best.last_result['accuracy']}\")\n",
    "print(f\"Best trial final validation f1score: {best.last_result['f1score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = result.results_df\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(\"vindr_tune_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid for alpha and gamma parameters of focal loss\n",
    "ax, gy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(1, 5, 100))\n",
    "grid = griddata(\n",
    "    res_df[[\"config/alpha\", \"config/gamma\"]],\n",
    "    res_df[\"f1score\"],\n",
    "    (ax, gy),\n",
    "    method=\"linear\",\n",
    ")\n",
    "plt.imshow(grid, extent=(0, 1, 1, 5), aspect=\"auto\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid for learning rate and weight decay\n",
    "ax, gy = np.meshgrid(np.logspace(-5, -2, 100), np.logspace(-6, -3, 100))\n",
    "grid = griddata(\n",
    "    res_df[[\"config/lr\", \"config/decay\"]], res_df[\"f1score\"], (ax, gy), method=\"linear\"\n",
    ")\n",
    "plt.imshow(grid, extent=(-5, -2, -6, -3), aspect=\"auto\", origin=\"lower\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Weight Decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[[\"config/alpha\", \"config/gamma\", \"f1score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamai2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
